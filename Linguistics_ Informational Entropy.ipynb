{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_zPyTMyxDgIE"
   },
   "source": [
    "# Informational Entropy\n",
    "\n",
    "SPS Blog: [The Things That Need Not Be Said: The Redundancy of Languages](https://sps.nus.edu.sg/blog/2021/07/19/the-things-that/)\n",
    "\n",
    "Link to Stanford Write-Up: [Claude Shannon & Information Theory](https://cs.stanford.edu/people/eroberts/courses/soco/projects/1999-00/information-theory/entropy_of_english_9.html#:~:text=Shannon%20uses%20an%20approximating%20function,is%202.62%20bits%20per%20letter.&text=Therefore%2C%20using%20the%20formula%201,estimate%20the%20redundancy%20of%20English.)\n",
    "\n",
    "Original Paper: [The Prediction and Entropy of Printed English](https://www.princeton.edu/~wbialek/rome/refs/shannon_51.pdf)\n",
    "\n",
    "***\n",
    "\n",
    "In information theory and linguistics, there is an area of study that allows us to measure the amount of information in a language. Shannon entropy, or information entropy, is a way of determining the amount of information, even without knowledge of the content of the message. This allows us to find interesting correlations with a language's information complexity, distinguish encrpted messages from static noise, as well as determine the information capacity of non-human linguistics.\n",
    "\n",
    "In this project, we will look at Claude Shannon's analysis of the redundancy in written english."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVxwonpZMx0Y"
   },
   "source": [
    "# Background Information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8GKmiEWEE6m"
   },
   "source": [
    "## Information Entropy\n",
    "\n",
    "The information entropy is a measure of how much information exists in a system. Another way we can think about it is the amount of surprise we get by seeing its outcome. It is given by the following formula,\n",
    "\n",
    "$$\n",
    "H(X) = -\\sum^{n}_{i=1} p(x_i) \\log_2 p(x_i)\n",
    "$$\n",
    "\n",
    "where the entropy $H$ of an event $X$ is given by the sum over the possible outcomes $x_i$ and their associated probabilities $p(x_i)$.\n",
    "\n",
    "Consider a fair coin flip - it can land either heads or tails with 50% chance. The entropy of the coin is then\n",
    "\n",
    "\\begin{align}\n",
    "H(X_\\mathrm{coin}) \n",
    "&= - p(x_\\mathrm{heads})\\log_2 p(x_\\mathrm{heads}) - p(x_\\mathrm{tails})\\log_2 p(x_\\mathrm{tails}) \\\\\n",
    "&= - \\frac{1}{2} \\log_2 \\left(\\frac{1}{2}\\right) \\times 2 \\\\\n",
    "&= - \\log_2 \\left(\\frac{1}{2}\\right)\n",
    "\\end{align}\n",
    "\n",
    "Similarly, we can find the entropy of a fair dice roll:\n",
    "\n",
    "$$\n",
    "H(X_\\mathrm{dice}) = - \\log_2{\\left(\\frac{1}{6}\\right)}\n",
    "$$\n",
    "\n",
    "For a completely biased coin toss (100% heads), we find that the entropy is 0. We can see that a biased coin will unsurprisingly always give heads. On the other hand, a fair coin will give heads or tails evenly, which has the maximum amount of surprise. The graph below illustrates this idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4GCQubPJV3d"
   },
   "source": [
    "![image.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQkTkYCNMtWT"
   },
   "source": [
    "## Shannon's Analysis\n",
    "\n",
    "Shannon showed that the lower the entropy of a language, the more information it carries. To find the entropy of a langauge, one way we can do this is consider the amount of surprise each character gives us.\n",
    "\n",
    "In a uniform random distribution of letters, we find that the informational entropy is\n",
    "\n",
    "$$\n",
    "H(X_\\mathrm{char}) = - \\log_2{\\left(\\frac{1}{26}\\right)}\n",
    "$$\n",
    "\n",
    "which will give us the highest possible entropy. It is equivalent to utter gibberish in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 473,
     "status": "ok",
     "timestamp": 1644696256208,
     "user": {
      "displayName": "Kai Xiang Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgBJUAJUYel5rdbTnU92LBd__AiaBgIY-k-dk5xLA=s64",
      "userId": "12776711516058684946"
     },
     "user_tz": -480
    },
    "id": "bnCnVVV2Sdeu",
    "outputId": "5c063c98-563b-47b8-c1a7-2b1d9b9971cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.700439718141092"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "-np.log2(1/26)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04vRH3y8Sc1y"
   },
   "source": [
    "However, the actual entropy for English is much lower!\n",
    "\n",
    "If you picked a random character out of a book, what would it most likely be? It would likely be \"e\", or perhaps a \"t\", and unlikely a \"q\". This tells us that there is some non-random distribution of alphabets in the English language. What if if you knew the preceeding letter was a \"q\"? Then you would know for certain that the next letter is a \"u\". \n",
    "\n",
    "Shannon's idea is to consider the probability of each character appearing in a text, given the preceeding letters. This allows us to find the entropy of the language as a measure of how well can the next letter be predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZ1tizpreTgo"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XcBgeluxOQFe"
   },
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hs89iqYOSWM"
   },
   "source": [
    "To model this, we will go over the letters in a text sample, and count the number of occurances of each letter. The counts can then be used to find the probabilities of each letter occuring. In our example, we will consider a 26-character alphabet, without spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldmyjmeOO0Pw"
   },
   "source": [
    "## Setting up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0EDTzsIUO29E"
   },
   "source": [
    "### Libraries\n",
    "\n",
    "To start off, we will import the needed libraries. We will use \n",
    "\n",
    "* `numpy` for math\n",
    "* `matplotlib.pyplot` for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1644696257085,
     "user": {
      "displayName": "Kai Xiang Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgBJUAJUYel5rdbTnU92LBd__AiaBgIY-k-dk5xLA=s64",
      "userId": "12776711516058684946"
     },
     "user_tz": -480
    },
    "id": "Jni_MloODcme"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hExTFGJOlU3"
   },
   "source": [
    "### Sample Text\n",
    "\n",
    "Next, we will need our text sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1644696257085,
     "user": {
      "displayName": "Kai Xiang Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgBJUAJUYel5rdbTnU92LBd__AiaBgIY-k-dk5xLA=s64",
      "userId": "12776711516058684946"
     },
     "user_tz": -480
    },
    "id": "PiQKCsDMJNzE"
   },
   "outputs": [],
   "source": [
    "text = \"\"\"Python is an interpreted high-level general-purpose programming language. Its design philosophy emphasizes code readability with its use of significant indentation. Its language constructs as well as its object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.[30]\n",
    "\n",
    "Python is dynamically-typed and garbage-collected. It supports multiple programming paradigms, including structured (particularly, procedural), object-oriented and functional programming. It is often described as a \"batteries included\" language due to its comprehensive standard library.[31][32]\n",
    "\n",
    "Guido van Rossum began working on Python in the late 1980s, as a successor to the ABC programming language, and first released it in 1991 as Python 0.9.0.[33] Python 2.0 was released in 2000 and introduced new features, such as list comprehensions and a cycle-detecting garbage collection system (in addition to reference counting). Python 3.0 was released in 2008 and was a major revision of the language that is not completely backward-compatible. Python 2 was discontinued with version 2.7.18 in 2020.[34]\n",
    "\n",
    "Python consistently ranks as one of the most popular programming languages\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yFKBJuT9Oyp0"
   },
   "source": [
    "### Cleaning Up\n",
    "\n",
    "As we can see, the text needs to be cleaned up before we start counting. Punctuation, as well as numbers, need to be removed first. We will also change all letters to lowercase. We will write a function to do this for us. \n",
    "\n",
    "It will take in a string as an input, and output a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1644696257085,
     "user": {
      "displayName": "Kai Xiang Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgBJUAJUYel5rdbTnU92LBd__AiaBgIY-k-dk5xLA=s64",
      "userId": "12776711516058684946"
     },
     "user_tz": -480
    },
    "id": "29TcgP2mPRZz",
    "outputId": "06283b92-f669-4475-d81a-c4eeecf3d1a9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'teststringwithsomepunctuationsaswellasnumberor'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean(dirty_text):              # Accepts a string as an argument\n",
    "    cleaned_text = \"\"               # Defines a variable to hold our cleaned text\n",
    "\n",
    "    for char in dirty_text:         # Lets' loop over the characters in the string\n",
    "        char = char.lower()         # Changes the letter to lowercase\n",
    "        if char.isalpha():          # Check if its alphabetical\n",
    "            cleaned_text += char    # and add it to the cleaned text if so\n",
    "    \n",
    "    return cleaned_text             # We then return our cleaned text\n",
    "\n",
    "clean(\"Test String, with some punctuations, as well as 1 number or 2?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gdql6aJiSogk"
   },
   "source": [
    "## First-Order Analysis\n",
    "\n",
    "We consider the entropy of picking a certain letter, without knowledge of any other characters around it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPdpfUqJQWI8"
   },
   "source": [
    "We can find the counts of the number of each alphabet in the text. We will use a for-loop to go over each character.\n",
    "\n",
    "We will need a variable to hold the total number of characters in the string, as well as a dictionary, to hold the number of counts of each letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1644696257086,
     "user": {
      "displayName": "Kai Xiang Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgBJUAJUYel5rdbTnU92LBd__AiaBgIY-k-dk5xLA=s64",
      "userId": "12776711516058684946"
     },
     "user_tz": -480
    },
    "id": "LuirMOXlQihQ"
   },
   "outputs": [],
   "source": [
    "total_count = 0                         # Total counts of the characters\n",
    "char_counts = {}                        # Dictionary to hold each character\n",
    "\n",
    "cleaned_text = clean(text)              # First, we clean up the text\n",
    "\n",
    "for char in cleaned_text:               # Going over each character\n",
    "    total_count += 1                    # We add to the total count\n",
    "\n",
    "    if char in char_counts:             # Check if the character is in the dictionary\n",
    "        char_counts[char] += 1          # If yes, we add +1 to it\n",
    "    else:                               # Otherwise\n",
    "        char_counts[char] = 1           # We initialise the character w/ a value of 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yzhCPl3aRRzo"
   },
   "source": [
    "Let's take a look at the distribution of characters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1644696257086,
     "user": {
      "displayName": "Kai Xiang Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgBJUAJUYel5rdbTnU92LBd__AiaBgIY-k-dk5xLA=s64",
      "userId": "12776711516058684946"
     },
     "user_tz": -480
    },
    "id": "4XQN-fcHRUhW",
    "outputId": "c81b896b-79e4-400e-b5ee-5615d48c3f66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 942\n",
      "The letter p appeared 37 times!\n",
      "The letter y appeared 19 times!\n",
      "The letter t appeared 72 times!\n",
      "The letter h appeared 25 times!\n",
      "The letter o appeared 65 times!\n",
      "The letter n appeared 76 times!\n",
      "The letter i appeared 72 times!\n",
      "The letter s appeared 66 times!\n",
      "The letter a appeared 87 times!\n",
      "The letter e appeared 93 times!\n",
      "The letter r appeared 61 times!\n",
      "The letter d appeared 41 times!\n",
      "The letter g appeared 40 times!\n",
      "The letter l appeared 47 times!\n",
      "The letter v appeared 5 times!\n",
      "The letter u appeared 28 times!\n",
      "The letter m appeared 26 times!\n",
      "The letter z appeared 1 times!\n",
      "The letter c appeared 41 times!\n",
      "The letter b appeared 12 times!\n",
      "The letter w appeared 11 times!\n",
      "The letter f appeared 10 times!\n",
      "The letter j appeared 4 times!\n",
      "The letter k appeared 3 times!\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of characters: {total_count}\")\n",
    "\n",
    "for char,count in char_counts.items():\n",
    "    print(f\"The letter {char} appeared {count} times!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wgTijL5-TA8L"
   },
   "source": [
    "We can see that the letter \"e\" is the most common, while \"q\" and \"z\" are practically non-existent. We can write a function to find our informational entropy: it will take in the total count and the dictionary as input, and return a number (float)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 430,
     "status": "ok",
     "timestamp": 1644696468626,
     "user": {
      "displayName": "Kai Xiang Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgBJUAJUYel5rdbTnU92LBd__AiaBgIY-k-dk5xLA=s64",
      "userId": "12776711516058684946"
     },
     "user_tz": -480
    },
    "id": "AcgEG2IkWyWL",
    "outputId": "9a76c8e2-a190-4d0a-d00f-cb36bf79680b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def entropy(total, dict_of_values):                 # Takes in a integer and a dictionary\n",
    "    H = 0                                           # Initialise a variable to hold the entropy\n",
    "\n",
    "    for char, count in dict_of_values.items():      # Going through the dictionary\n",
    "        prob = count/total                          # Convert the counts into probability\n",
    "\n",
    "        H = H + - prob*np.log2(prob)                # Add it to the entropy\n",
    "    \n",
    "    return H                                        # And return it\n",
    "\n",
    "entropy(10, {\"heads\": 5, \"tails\": 5})               # Test the function using a fair coin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1644696257087,
     "user": {
      "displayName": "Kai Xiang Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgBJUAJUYel5rdbTnU92LBd__AiaBgIY-k-dk5xLA=s64",
      "userId": "12776711516058684946"
     },
     "user_tz": -480
    },
    "id": "FvLnqG1oXrU3",
    "outputId": "b9d99801-a62c-4763-e740-f752785325ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.165738466622712"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(total_count, char_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qbbWDnbIXxIG"
   },
   "source": [
    "We can see that the entropy is lower ($4.17 < 4.70$), since English has some structure in the distribution of letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cYGoiI2FX2G5"
   },
   "source": [
    "## Second-Order Analysis\n",
    "\n",
    "We can perform a second-order analysis, where we consider the previous character, and find the probability of the next character. This is known as finding the N-gram, where in this case N is 1. In this case, we must use a different formula for information entropy:\n",
    "\n",
    "$$\n",
    "H(X) = p(x_\\mathrm{N-gram}) \\log_2 p(x_i|x_\\mathrm{prev})\n",
    "$$\n",
    "\n",
    "where in the case of N-grams, we have the probability $p(x_\\mathrm{N-gram})$ of the N-gram (or character pair) appearing, and the conditional probability $p(x_i|x_\\mathrm{prev})$ of the letter $x_i$ appearing, given that the sequence $x_\\mathrm{prev}$ came before it. \n",
    "\n",
    "In the case of considering only the previous character, we just need the probability of the previous character $p(x_\\mathrm{prev})$ to find this conditional probability\n",
    "\n",
    "$$\n",
    "p(x_i|x_\\mathrm{prev}) = \\frac{p(x_i)\\cap p(x_\\mathrm{prev})}{p(x_\\mathrm{prev})}\n",
    "$$\n",
    "\n",
    "The information entropy becomes\n",
    "\n",
    "$$\n",
    "H(X) = p(x_\\mathrm{N-gram}) \\log_2{\\left(\\frac{p(x_i)\\cap p(x_\\mathrm{prev})}{p(x_\\mathrm{prev})}\\right)}\n",
    "$$\n",
    "\n",
    "We do the same procedure as before, but loop over two characters at once to find $p(x_i)\\cap(x_\\mathrm{prev})$. $p(x_\\mathrm{prev})$ was found using our earlier analysis. A modified entropy function will also be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1644696257087,
     "user": {
      "displayName": "Kai Xiang Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgBJUAJUYel5rdbTnU92LBd__AiaBgIY-k-dk5xLA=s64",
      "userId": "12776711516058684946"
     },
     "user_tz": -480
    },
    "id": "0KnTSYYeYOUe"
   },
   "outputs": [],
   "source": [
    "total_count2 = 0                        # Total counts of the character pair\n",
    "char_counts2 = {}                       # Dictionary to hold each character pair\n",
    "\n",
    "cleaned_text = clean(text)              # First, we clean up the text\n",
    "length_of_text = len(cleaned_text)\n",
    "\n",
    "for i in range(length_of_text - 1):     # Going over each character pair\n",
    "                                        # Note that -1 is to prevent overshoot at the end\n",
    "\n",
    "    char_pair = cleaned_text[i:i+2]     # For each pair,\n",
    "    total_count2 += 1                   # We add to the total count\n",
    "\n",
    "    if char_pair in char_counts2:       # Check if the pair is in the dictionary\n",
    "        char_counts2[char_pair] += 1    # If yes, we add +1 to it\n",
    "    else:                               # Otherwise\n",
    "        char_counts2[char_pair] = 1     # We initialise the pair w/ a value of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1644696257087,
     "user": {
      "displayName": "Kai Xiang Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgBJUAJUYel5rdbTnU92LBd__AiaBgIY-k-dk5xLA=s64",
      "userId": "12776711516058684946"
     },
     "user_tz": -480
    },
    "id": "gkF-i9PyZImo",
    "outputId": "69f5b2bc-7a4d-402c-94a1-e5e0ef94d9fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 941\n",
      "The letter py appeared 8 times!\n",
      "The letter yt appeared 9 times!\n",
      "The letter th appeared 15 times!\n",
      "The letter ho appeared 8 times!\n",
      "The letter on appeared 20 times!\n",
      "The letter ni appeared 7 times!\n",
      "The letter is appeared 8 times!\n",
      "The letter sa appeared 8 times!\n",
      "The letter an appeared 19 times!\n",
      "The letter in appeared 21 times!\n",
      "The letter nt appeared 11 times!\n",
      "The letter te appeared 13 times!\n",
      "The letter er appeared 7 times!\n",
      "The letter rp appeared 3 times!\n",
      "The letter pr appeared 12 times!\n",
      "The letter re appeared 12 times!\n",
      "The letter et appeared 5 times!\n",
      "The letter ed appeared 16 times!\n",
      "The letter dh appeared 1 times!\n",
      "The letter hi appeared 3 times!\n",
      "The letter ig appeared 4 times!\n",
      "The letter gh appeared 1 times!\n",
      "The letter hl appeared 1 times!\n",
      "The letter le appeared 12 times!\n",
      "The letter ev appeared 2 times!\n",
      "The letter ve appeared 3 times!\n",
      "The letter el appeared 9 times!\n",
      "The letter lg appeared 1 times!\n",
      "The letter ge appeared 10 times!\n",
      "The letter en appeared 9 times!\n",
      "The letter ne appeared 3 times!\n",
      "The letter ra appeared 11 times!\n",
      "The letter al appeared 7 times!\n",
      "The letter lp appeared 3 times!\n",
      "The letter pu appeared 2 times!\n",
      "The letter ur appeared 4 times!\n",
      "The letter po appeared 3 times!\n",
      "The letter os appeared 4 times!\n",
      "The letter se appeared 5 times!\n",
      "The letter ep appeared 4 times!\n",
      "The letter ro appeared 11 times!\n",
      "The letter og appeared 7 times!\n",
      "The letter gr appeared 6 times!\n",
      "The letter am appeared 8 times!\n",
      "The letter mm appeared 6 times!\n",
      "The letter mi appeared 7 times!\n",
      "The letter ng appeared 15 times!\n",
      "The letter gl appeared 3 times!\n",
      "The letter la appeared 12 times!\n",
      "The letter gu appeared 7 times!\n",
      "The letter ua appeared 6 times!\n",
      "The letter ag appeared 8 times!\n",
      "The letter ei appeared 1 times!\n",
      "The letter it appeared 13 times!\n",
      "The letter ts appeared 9 times!\n",
      "The letter sd appeared 3 times!\n",
      "The letter de appeared 7 times!\n",
      "The letter es appeared 10 times!\n",
      "The letter si appeared 11 times!\n",
      "The letter gn appeared 2 times!\n",
      "The letter np appeared 4 times!\n",
      "The letter ph appeared 3 times!\n",
      "The letter il appeared 2 times!\n",
      "The letter lo appeared 3 times!\n",
      "The letter so appeared 5 times!\n",
      "The letter op appeared 2 times!\n",
      "The letter hy appeared 1 times!\n",
      "The letter ye appeared 1 times!\n",
      "The letter em appeared 3 times!\n",
      "The letter mp appeared 5 times!\n",
      "The letter ha appeared 4 times!\n",
      "The letter as appeared 16 times!\n",
      "The letter iz appeared 1 times!\n",
      "The letter ze appeared 1 times!\n",
      "The letter sc appeared 5 times!\n",
      "The letter co appeared 12 times!\n",
      "The letter od appeared 3 times!\n",
      "The letter ea appeared 8 times!\n",
      "The letter ad appeared 3 times!\n",
      "The letter da appeared 7 times!\n",
      "The letter ab appeared 3 times!\n",
      "The letter bi appeared 1 times!\n",
      "The letter li appeared 3 times!\n",
      "The letter ty appeared 2 times!\n",
      "The letter yw appeared 1 times!\n",
      "The letter wi appeared 2 times!\n",
      "The letter su appeared 5 times!\n",
      "The letter us appeared 1 times!\n",
      "The letter eo appeared 2 times!\n",
      "The letter of appeared 4 times!\n",
      "The letter fs appeared 1 times!\n",
      "The letter if appeared 1 times!\n",
      "The letter fi appeared 2 times!\n",
      "The letter ic appeared 4 times!\n",
      "The letter ca appeared 4 times!\n",
      "The letter ti appeared 14 times!\n",
      "The letter nd appeared 10 times!\n",
      "The letter ta appeared 2 times!\n",
      "The letter at appeared 6 times!\n",
      "The letter io appeared 7 times!\n",
      "The letter sl appeared 2 times!\n",
      "The letter ec appeared 11 times!\n",
      "The letter ns appeared 6 times!\n",
      "The letter st appeared 8 times!\n",
      "The letter tr appeared 4 times!\n",
      "The letter ru appeared 2 times!\n",
      "The letter uc appeared 5 times!\n",
      "The letter ct appeared 9 times!\n",
      "The letter sw appeared 2 times!\n",
      "The letter we appeared 1 times!\n",
      "The letter ll appeared 5 times!\n",
      "The letter ob appeared 2 times!\n",
      "The letter bj appeared 2 times!\n",
      "The letter je appeared 3 times!\n",
      "The letter to appeared 6 times!\n",
      "The letter or appeared 8 times!\n",
      "The letter ri appeared 5 times!\n",
      "The letter ie appeared 3 times!\n",
      "The letter ap appeared 1 times!\n",
      "The letter pp appeared 3 times!\n",
      "The letter oa appeared 1 times!\n",
      "The letter ac appeared 3 times!\n",
      "The letter ch appeared 2 times!\n",
      "The letter ai appeared 1 times!\n",
      "The letter im appeared 1 times!\n",
      "The letter mt appeared 1 times!\n",
      "The letter oh appeared 1 times!\n",
      "The letter he appeared 7 times!\n",
      "The letter me appeared 1 times!\n",
      "The letter rs appeared 4 times!\n",
      "The letter wr appeared 1 times!\n",
      "The letter cl appeared 4 times!\n",
      "The letter ar appeared 11 times!\n",
      "The letter rl appeared 2 times!\n",
      "The letter gi appeared 2 times!\n",
      "The letter lc appeared 1 times!\n",
      "The letter ef appeared 2 times!\n",
      "The letter fo appeared 1 times!\n",
      "The letter sm appeared 2 times!\n",
      "The letter ma appeared 2 times!\n",
      "The letter dl appeared 3 times!\n",
      "The letter rg appeared 1 times!\n",
      "The letter oj appeared 1 times!\n",
      "The letter sp appeared 2 times!\n",
      "The letter dy appeared 1 times!\n",
      "The letter yn appeared 1 times!\n",
      "The letter na appeared 6 times!\n",
      "The letter ly appeared 4 times!\n",
      "The letter yp appeared 2 times!\n",
      "The letter pe appeared 1 times!\n",
      "The letter dg appeared 1 times!\n",
      "The letter ga appeared 3 times!\n",
      "The letter rb appeared 2 times!\n",
      "The letter ba appeared 4 times!\n",
      "The letter ol appeared 2 times!\n",
      "The letter di appeared 9 times!\n",
      "The letter up appeared 1 times!\n",
      "The letter rt appeared 3 times!\n",
      "The letter mu appeared 1 times!\n",
      "The letter ul appeared 3 times!\n",
      "The letter lt appeared 1 times!\n",
      "The letter ip appeared 1 times!\n",
      "The letter pl appeared 2 times!\n",
      "The letter gp appeared 2 times!\n",
      "The letter pa appeared 3 times!\n",
      "The letter gm appeared 1 times!\n",
      "The letter ms appeared 1 times!\n",
      "The letter nc appeared 5 times!\n",
      "The letter lu appeared 2 times!\n",
      "The letter ud appeared 2 times!\n",
      "The letter gs appeared 1 times!\n",
      "The letter tu appeared 2 times!\n",
      "The letter dp appeared 1 times!\n",
      "The letter cu appeared 1 times!\n",
      "The letter oc appeared 1 times!\n",
      "The letter ce appeared 4 times!\n",
      "The letter du appeared 3 times!\n",
      "The letter df appeared 2 times!\n",
      "The letter fu appeared 1 times!\n",
      "The letter un appeared 2 times!\n",
      "The letter ft appeared 3 times!\n",
      "The letter cr appeared 1 times!\n",
      "The letter ib appeared 3 times!\n",
      "The letter be appeared 2 times!\n",
      "The letter tt appeared 1 times!\n",
      "The letter ue appeared 2 times!\n",
      "The letter oi appeared 1 times!\n",
      "The letter om appeared 4 times!\n",
      "The letter eh appeared 2 times!\n",
      "The letter iv appeared 1 times!\n",
      "The letter rd appeared 2 times!\n",
      "The letter br appeared 1 times!\n",
      "The letter ry appeared 1 times!\n",
      "The letter yg appeared 1 times!\n",
      "The letter ui appeared 1 times!\n",
      "The letter id appeared 1 times!\n",
      "The letter do appeared 1 times!\n",
      "The letter ov appeared 1 times!\n",
      "The letter va appeared 1 times!\n",
      "The letter nr appeared 1 times!\n",
      "The letter ss appeared 3 times!\n",
      "The letter um appeared 1 times!\n",
      "The letter mb appeared 1 times!\n",
      "The letter eg appeared 1 times!\n",
      "The letter nw appeared 4 times!\n",
      "The letter wo appeared 1 times!\n",
      "The letter rk appeared 1 times!\n",
      "The letter ki appeared 1 times!\n",
      "The letter go appeared 1 times!\n",
      "The letter cc appeared 1 times!\n",
      "The letter ot appeared 2 times!\n",
      "The letter bc appeared 1 times!\n",
      "The letter cp appeared 1 times!\n",
      "The letter ir appeared 1 times!\n",
      "The letter wa appeared 5 times!\n",
      "The letter sr appeared 2 times!\n",
      "The letter dn appeared 1 times!\n",
      "The letter ew appeared 1 times!\n",
      "The letter wf appeared 1 times!\n",
      "The letter fe appeared 2 times!\n",
      "The letter tc appeared 2 times!\n",
      "The letter cy appeared 1 times!\n",
      "The letter yc appeared 1 times!\n",
      "The letter gg appeared 1 times!\n",
      "The letter sy appeared 1 times!\n",
      "The letter ys appeared 1 times!\n",
      "The letter dd appeared 1 times!\n",
      "The letter ou appeared 1 times!\n",
      "The letter dw appeared 2 times!\n",
      "The letter aj appeared 1 times!\n",
      "The letter jo appeared 1 times!\n",
      "The letter rr appeared 1 times!\n",
      "The letter vi appeared 1 times!\n",
      "The letter no appeared 2 times!\n",
      "The letter sn appeared 1 times!\n",
      "The letter yb appeared 1 times!\n",
      "The letter ck appeared 1 times!\n",
      "The letter kw appeared 1 times!\n",
      "The letter dc appeared 1 times!\n",
      "The letter bl appeared 1 times!\n",
      "The letter nu appeared 1 times!\n",
      "The letter hv appeared 1 times!\n",
      "The letter tl appeared 1 times!\n",
      "The letter yr appeared 1 times!\n",
      "The letter nk appeared 1 times!\n",
      "The letter ks appeared 1 times!\n",
      "The letter mo appeared 1 times!\n",
      "The letter tp appeared 1 times!\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of characters: {total_count2}\")\n",
    "\n",
    "for pair,count in char_counts2.items():\n",
    "    print(f\"The letter {pair} appeared {count} times!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "executionInfo": {
     "elapsed": 317,
     "status": "ok",
     "timestamp": 1644697687695,
     "user": {
      "displayName": "Kai Xiang Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgBJUAJUYel5rdbTnU92LBd__AiaBgIY-k-dk5xLA=s64",
      "userId": "12776711516058684946"
     },
     "user_tz": -480
    },
    "id": "RU5-CZRfmomU"
   },
   "outputs": [],
   "source": [
    "def entropy_Ngram(total_Ngram, dict_of_Ngrams, total, dict_of_counts):\n",
    "    # Our function takes in the new total counts,\n",
    "    # the counts of the character pairs,\n",
    "    # and the total and counts we found earlier\n",
    "\n",
    "    H = 0                                           # Initialise a variable to hold the entropy\n",
    "\n",
    "    for pair, count in dict_of_Ngrams.items():      # Going through the dictionary of pairs\n",
    "        # Consider the character pair\n",
    "        prob = count/total_Ngram                    # Convert the counts into probability\n",
    "\n",
    "        # Consider the preceeding character\n",
    "        prev_char = pair[0]                         # Find the previous character\n",
    "        prev_count = dict_of_counts[prev_char]      # Find the number of counts it had\n",
    "        prev_prob = prev_count/total                # And so, find its probability\n",
    "\n",
    "        # Hence, find the conditional probability\n",
    "        cond_prob = prob / prev_prob                # Finding the conditional probability\n",
    "\n",
    "        # Entropy\n",
    "        H = H + - prob*np.log2(cond_prob)           # Add it to the entropy\n",
    "    \n",
    "    return H                                        # And return it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1644697689247,
     "user": {
      "displayName": "Kai Xiang Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgBJUAJUYel5rdbTnU92LBd__AiaBgIY-k-dk5xLA=s64",
      "userId": "12776711516058684946"
     },
     "user_tz": -480
    },
    "id": "aEd_XfBZZTDg",
    "outputId": "4a8e0219-302c-4bdb-c78c-99ea1d93862b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1930566395509135"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy_Ngram(total_count2, char_counts2, total_count, char_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Xhz0OfoZWNS"
   },
   "source": [
    "And we can see that the entropy is much lower than before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFUVy1qmZZGe"
   },
   "source": [
    "# Extensions\n",
    "\n",
    "The information entropy goes down when we consider even more characters preceeding. We can also consider an analysis of word probabilities, as words also hold meaning and structure in a language.\n",
    "\n",
    "1. Use a larger text sample. Does it give the same result?\n",
    "2. How do names affect the results, instead of common vernecular words?\n",
    "3. Consider more previous characters in the analysis. How much lower can the informational entropy go?\n",
    "4. Consider an analysis of words. How low is the entropy?\n",
    "5. Using another language, find the informational entropy\n",
    "6. Compare a text sample translated in two languages. Is the informational entropy the same?\n",
    "7. Compare a text message encrypted using various protocols. Which protocols can mask their information signature? Can we circumvent it?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMBHTmDR+ea/RVoexcuANvN",
   "collapsed_sections": [],
   "name": "Linguistics: Informational Entropy.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
